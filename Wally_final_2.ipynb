{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWPl7oQSgNGdjQZGMofInN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdarshSRM/FirstSite/blob/main/Wally_final_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1U4iterNsza",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Create the text box and button\n",
        "url_input = widgets.Text(placeholder='Paste URL here', description='URL:')\n",
        "button = widgets.Button(description=\"Set URL\", button_style='success')\n",
        "output = widgets.Output()\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    global TARGET_URL\n",
        "    TARGET_URL = url_input.value.strip()\n",
        "    with output:\n",
        "        output.clear_output()\n",
        "        if TARGET_URL:\n",
        "            print(f\"‚úÖ Set to: {TARGET_URL}\")\n",
        "        else:\n",
        "            print(\"‚ùå Please paste a URL first.\")\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "\n",
        "# Display everything\n",
        "display(url_input, button, output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "to_remove = ['downloads', 'imagebam_links.txt', 'result_images.zip']\n",
        "\n",
        "for item in to_remove:\n",
        "    if os.path.exists(item):\n",
        "        if os.path.isdir(item):\n",
        "            shutil.rmtree(item)\n",
        "            print(f\"Removed folder: {item}\")\n",
        "        else:\n",
        "            os.remove(item)\n",
        "            print(f\"Removed file: {item}\")\n",
        "\n",
        "print(\"Cleanup complete. Ready for new extraction.\")"
      ],
      "metadata": {
        "id": "oQ9U9HT9N2DM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def scrape_links():\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
        "    print(f\"Fetching links from: {TARGET_URL}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(TARGET_URL, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        flow_a_normal = []\n",
        "        flow_b_imagebam = []\n",
        "        flow_c_thumbs = []\n",
        "\n",
        "        # Find all <img> tags for normal images and thumbnails\n",
        "        img_tags = soup.find_all('img')\n",
        "        for img in img_tags:\n",
        "            src = img.get('src')\n",
        "            if not src: continue\n",
        "\n",
        "            full_url = urljoin(TARGET_URL, src)\n",
        "            if \"thumbs2.imagebam.com\" in full_url:\n",
        "                flow_c_thumbs.append(full_url)\n",
        "            elif any(ext in full_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):\n",
        "                flow_a_normal.append(full_url)\n",
        "\n",
        "        # Find all <a> tags for ImageBam pages\n",
        "        bam_links = [a['href'] for a in soup.find_all('a', href=True) if \"imagebam.com/image/\" in a['href']]\n",
        "        flow_b_imagebam.extend(bam_links)\n",
        "\n",
        "        # Merge while maintaining uniqueness\n",
        "        all_links = flow_a_normal + flow_c_thumbs + flow_b_imagebam\n",
        "        unique_links = list(dict.fromkeys(all_links))\n",
        "\n",
        "        print(\"-\" * 30)\n",
        "        print(f\"üìä Extraction Summary:\")\n",
        "        print(f\"‚úÖ Normal Images:    {len(set(flow_a_normal))}\")\n",
        "        print(f\"‚úÖ IB Thumbnails:    {len(set(flow_c_thumbs))}\")\n",
        "        print(f\"‚úÖ IB Full Pages:    {len(set(flow_b_imagebam))}\")\n",
        "        print(f\"üì¶ Total Unique:     {len(unique_links)}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        if unique_links:\n",
        "            with open(\"imagebam_links.txt\", \"w\") as f:\n",
        "                for link in unique_links:\n",
        "                    f.write(link + \"\\n\")\n",
        "            print(f\"File updated successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "scrape_links()"
      ],
      "metadata": {
        "id": "bHxll8qgN3_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import time\n",
        "\n",
        "os.makedirs(\"downloads\", exist_ok=True)\n",
        "\n",
        "def resolve_imagebam_link(session, url):\n",
        "    url = url.replace(\"http://\", \"https://\")\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\", \"Referer\": \"https://www.imagebam.com/\"}\n",
        "    try:\n",
        "        session.cookies.set(\"sfw_inter\", \"1\", domain=\".imagebam.com\")\n",
        "        session.cookies.set(\"nsfw_inter\", \"1\", domain=\".imagebam.com\")\n",
        "        response = session.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        img = soup.find('img', id='main-image') or soup.select_one('img.main-image') or soup.select_one('img[src*=\"images2.imagebam.com\"]')\n",
        "        return img.get('src') if img else None\n",
        "    except: return None\n",
        "\n",
        "def download_images():\n",
        "    # RESET STATS AND COUNTERS\n",
        "    stats = {\"normal\": 0, \"ib_full\": 0, \"ib_thumb\": 0, \"failed\": 0}\n",
        "\n",
        "    # SEPARATE COUNTERS FOR NAMING (Start at 1)\n",
        "    thumb_counter = 1\n",
        "    full_ib_counter = 1\n",
        "\n",
        "    if not os.path.exists(\"imagebam_links.txt\"):\n",
        "        print(\"Error: imagebam_links.txt not found!\")\n",
        "        return\n",
        "\n",
        "    with open(\"imagebam_links.txt\", \"r\") as f:\n",
        "        urls = [line.strip() for line in f.readlines() if line.strip()]\n",
        "\n",
        "    session = requests.Session()\n",
        "    print(f\"üöÄ Starting download session...\")\n",
        "\n",
        "    for index, url in enumerate(urls, start=1):\n",
        "        try:\n",
        "            print(f\"[{index}/{len(urls)}] Processing: {url[:50]}...\")\n",
        "\n",
        "            is_ib_thumb = \"thumbs2.imagebam.com\" in url\n",
        "            is_ib_full = \"imagebam.com/image/\" in url\n",
        "\n",
        "            if is_ib_thumb:\n",
        "                # CASE 1: THUMBNAILS (Uses thumb_counter)\n",
        "                final_direct_url = url\n",
        "                referer = TARGET_URL if 'TARGET_URL' in globals() else url\n",
        "                original_id = url.split('/')[-1].split('?')[0]\n",
        "                filename = f\"downloads/imagebam_thumb_{thumb_counter}_{original_id}\"\n",
        "                thumb_counter += 1\n",
        "                case_key = \"ib_thumb\"\n",
        "\n",
        "            elif is_ib_full:\n",
        "                # CASE 2: FULL PAGES (Uses full_ib_counter)\n",
        "                final_direct_url = resolve_imagebam_link(session, url)\n",
        "                referer = url\n",
        "                original_id = url.split('/')[-1].split('?')[0]\n",
        "                filename = f\"downloads/{full_ib_counter}-{original_id}\"\n",
        "                full_ib_counter += 1\n",
        "                case_key = \"ib_full\"\n",
        "\n",
        "            else:\n",
        "                # CASE 3: NORMAL IMAGES (Original Name)\n",
        "                final_direct_url = url\n",
        "                referer = TARGET_URL if 'TARGET_URL' in globals() else url\n",
        "                filename = f\"downloads/{url.split('/')[-1].split('?')[0]}\"\n",
        "                case_key = \"normal\"\n",
        "\n",
        "            if final_direct_url:\n",
        "                img_res = session.get(final_direct_url, headers={\"Referer\": referer})\n",
        "\n",
        "                # Simple Extension fix\n",
        "                ext = \".jpg\"\n",
        "                if \".png\" in final_direct_url.lower(): ext = \".png\"\n",
        "                elif \".webp\" in final_direct_url.lower(): ext = \".webp\"\n",
        "\n",
        "                if not filename.lower().endswith(('.jpg', '.jpeg', '.png', '.webp')):\n",
        "                    filename += ext\n",
        "\n",
        "                with open(filename, 'wb') as f:\n",
        "                    f.write(img_res.content)\n",
        "\n",
        "                print(f\"   -> ‚úÖ Saved: {os.path.basename(filename)}\")\n",
        "                stats[case_key] += 1\n",
        "            else:\n",
        "                stats[\"failed\"] += 1\n",
        "\n",
        "            time.sleep(0.7)\n",
        "        except Exception as e:\n",
        "            print(f\"   -> ‚ùå Error: {e}\")\n",
        "            stats[\"failed\"] += 1\n",
        "\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"üèÅ SESSION COMPLETE\")\n",
        "    print(f\"üñºÔ∏è ImageBam Full Count:   {stats['ib_full']}\")\n",
        "    print(f\"üñºÔ∏è ImageBam Thumb Count: {stats['ib_thumb']}\")\n",
        "    print(f\"üì∏ Normal Images Count:   {stats['normal']}\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    download_images()"
      ],
      "metadata": {
        "id": "lu15GPKJN7Wk",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "if os.path.exists(\"downloads\") and len(os.listdir(\"downloads\")) > 0:\n",
        "    print(\"Zipping files...\")\n",
        "    !zip -q -r result_images.zip downloads\n",
        "    print(\"Download starting...\")\n",
        "    files.download('result_images.zip')\n",
        "else:\n",
        "    print(\"Nothing to zip. Make sure the download cell (Cell 4) finished successfully.\")"
      ],
      "metadata": {
        "id": "VGcbLxVxN_oI",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}