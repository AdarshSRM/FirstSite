{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPd+n7wA0XWrR4rKJho9bYL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdarshSRM/FirstSite/blob/main/Wayback.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kO95koa8_6CP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# @title Wayback Machine Media Downloader with Fallback Recovery\n",
        "import os\n",
        "import requests\n",
        "import time\n",
        "import zipfile\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from google.colab import files\n",
        "\n",
        "def get_fallback_snapshots(original_url):\n",
        "    \"\"\"Queries the CDX API for all successful captures of a specific file.\"\"\"\n",
        "    cdx_url = \"http://web.archive.org/cdx/search/cdx\"\n",
        "    params = {\n",
        "        \"url\": original_url,\n",
        "        \"output\": \"json\",\n",
        "        \"fl\": \"timestamp,statuscode\",\n",
        "        \"filter\": \"statuscode:200\"\n",
        "    }\n",
        "    try:\n",
        "        r = requests.get(cdx_url, params=params, timeout=10)\n",
        "        if r.status_code == 200:\n",
        "            data = r.json()\n",
        "            if len(data) > 1:\n",
        "                # Return timestamps in reverse order (newest first)\n",
        "                return [row[0] for row in data[1:]][::-1]\n",
        "    except:\n",
        "        pass\n",
        "    return []\n",
        "\n",
        "def download_images(target_url):\n",
        "    print(f\"Scraping: {target_url}\\n\")\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "\n",
        "    try:\n",
        "        response = requests.get(target_url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        links = soup.find_all('a', href=True)\n",
        "        media_entries = []\n",
        "\n",
        "        for link in links:\n",
        "            href = link['href']\n",
        "            if '/media3/' in href and any(ext in href.lower() for ext in ['.jpg', '.jpeg', '.png', '.mp4']):\n",
        "                full_url = urljoin(target_url, href)\n",
        "                if full_url not in [m['url'] for m in media_entries]:\n",
        "                    # Extract original source URL from the Wayback wrapper\n",
        "                    parts = full_url.split('/http')\n",
        "                    original_src = 'http' + parts[1] if len(parts) > 1 else full_url\n",
        "                    media_entries.append({'url': full_url, 'original_src': original_src})\n",
        "\n",
        "        if not media_entries:\n",
        "            print(\"No high-res media links found.\")\n",
        "            return\n",
        "\n",
        "        folder_name = \"images\"\n",
        "        if not os.path.exists(folder_name): os.makedirs(folder_name)\n",
        "\n",
        "        downloaded_paths = []\n",
        "        failed_urls = []\n",
        "        recovered_count = 0\n",
        "\n",
        "        for item in media_entries:\n",
        "            url = item['url']\n",
        "            filename = os.path.basename(urlparse(url).path)\n",
        "            success = False\n",
        "\n",
        "            # Try primary download\n",
        "            try:\n",
        "                r = requests.get(url, headers=headers, stream=True, timeout=10)\n",
        "                if r.status_code == 200:\n",
        "                    success = True\n",
        "                else:\n",
        "                    print(f\"⚠️ Primary link failed ({r.status_code}) for {filename}. Checking history...\")\n",
        "            except:\n",
        "                print(f\"⚠️ Connection error for {filename}. Checking history...\")\n",
        "\n",
        "            # Fallback Logic: Search previous captures\n",
        "            if not success:\n",
        "                history = get_fallback_snapshots(item['original_src'])\n",
        "                for timestamp in history:\n",
        "                    fallback_url = f\"https://web.archive.org/web/{timestamp}/{item['original_src']}\"\n",
        "                    try:\n",
        "                        r = requests.get(fallback_url, headers=headers, stream=True, timeout=10)\n",
        "                        if r.status_code == 200:\n",
        "                            url = fallback_url\n",
        "                            success = True\n",
        "                            recovered_count += 1\n",
        "                            print(f\"✅ Recovered {filename} from snapshot: {timestamp}\")\n",
        "                            break\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "            if success:\n",
        "                filepath = os.path.join(folder_name, filename)\n",
        "                with open(filepath, 'wb') as f:\n",
        "                    for chunk in r.iter_content(1024): f.write(chunk)\n",
        "                downloaded_paths.append(filepath)\n",
        "                if not any(x in locals() for x in ['recovered_count']): print(f\"Downloaded: {filename}\")\n",
        "            else:\n",
        "                failed_urls.append(url)\n",
        "\n",
        "        # Zip it up\n",
        "        if downloaded_paths:\n",
        "            zip_path = \"/content/images.zip\"\n",
        "            with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "                for file in downloaded_paths:\n",
        "                    zipf.write(file, os.path.join(\"images\", os.path.basename(file)))\n",
        "            print(f\"\\n--- Process Complete ---\")\n",
        "            print(f\"Successfully downloaded: {len(downloaded_paths)} (Recovered from history: {recovered_count})\")\n",
        "            files.download(zip_path)\n",
        "\n",
        "        if failed_urls:\n",
        "            print(\"\\n\" + \"!\"*10 + \" FAILED URLS \" + \"!\"*10)\n",
        "            for f_url in failed_urls: print(f_url)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "# UI Code\n",
        "url_input = widgets.Text(value='https://web.archive.org/web/20150712230220/http://www.notiblog.com/soledad-cescato-prefiere-trio-y-partuza', layout=widgets.Layout(width='80%'))\n",
        "button = widgets.Button(description='Download Media', button_style='success')\n",
        "output = widgets.Output()\n",
        "def run(b):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        download_images(url_input.value)\n",
        "button.on_click(run)\n",
        "display(url_input, button, output)"
      ]
    }
  ]
}